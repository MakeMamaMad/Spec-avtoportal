import os
import json    
import random
import re
import html
from pathlib import Path
from datetime import datetime, timedelta, timezone

import requests
from dateutil import parser as dtparser


BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").strip()
NEWS_JSON_PATH = os.getenv("NEWS_JSON_PATH", "frontend/data/news.json").strip()
PICK_N = int(os.getenv("DIGEST_PICK_N", "6"))

STATE_PATH = Path("tools/daily_digest/state.json")
STATE_PATH.parent.mkdir(parents=True, exist_ok=True)

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äú—Å—Ç—Ä–æ–≥–æ —Ç—è–≥–∞—á–∏/–ø–æ–ª—É–ø—Ä–∏—Ü–µ–ø—ã‚Äù
TOPIC_WORDS = [
    "–ø–æ–ª—É–ø—Ä–∏—Ü–µ–ø", "–ø–æ–ª—É–ø—Ä–∏—Ü–µ–ø—ã", "–ø—Ä–∏—Ü–µ–ø", "–ø—Ä–∏—Ü–µ–ø—ã", "—Ç—è–≥–∞—á", "—Ç—è–≥–∞—á–∏",
    "—Å–µ–¥–µ–ª—å–Ω—ã–π", "—Å–µ–¥–µ–ª—å–Ω—ã–µ",
    "trailer", "trailers", "semi", "semi-trailer", "tractor trailer", "articulated",
]

def load_state():
    if STATE_PATH.exists():
        try:
            return json.loads(STATE_PATH.read_text(encoding="utf-8"))
        except Exception:
            pass
    return {"used_urls": [], "last_post_date": ""}

def save_state(state):
    STATE_PATH.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def read_news():
    p = Path(NEWS_JSON_PATH)
    if not p.exists():
        raise RuntimeError(f"news json not found: {NEWS_JSON_PATH}")
    data = json.loads(p.read_text(encoding="utf-8"))
    if not isinstance(data, list):
        raise RuntimeError("news.json must be a list")
    return data

def extract_url(item: dict) -> str:
    for k in ("url", "link", "href", "source_url"):
        v = item.get(k)
        if isinstance(v, str) and v.startswith("http"):
            return v.strip()
    return ""

def extract_title(item: dict) -> str:
    for k in ("title", "headline", "name"):
        v = item.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def extract_date(item: dict) -> datetime | None:
    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ–ª—è—Ö
    for k in ("published_at", "published", "date", "datetime", "time", "ts"):
        v = item.get(k)
        if not v:
            continue
        try:
            if isinstance(v, (int, float)):
                # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º unix seconds
                return datetime.fromtimestamp(float(v), tz=timezone.utc)
            if isinstance(v, str):
                d = dtparser.parse(v)
                if d.tzinfo is None:
                    d = d.replace(tzinfo=timezone.utc)
                return d
        except Exception:
            continue
    return None

def is_on_topic(item: dict) -> bool:
    title = extract_title(item).lower()
    if any(w in title for w in TOPIC_WORDS):
        return True
    # –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–≥–∏/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî —Ç–æ–∂–µ —É—á–∏—Ç—ã–≤–∞–µ–º
    tags = item.get("tags") or item.get("categories")
    if isinstance(tags, list):
        joined = " ".join([str(x).lower() for x in tags])
        if any(w in joined for w in TOPIC_WORDS):
            return True
    return False

def pick_items(news: list[dict], used_urls: set[str]) -> list[dict]:
    # —Å–≤–µ–∂–µ—Å—Ç—å: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 48 —á–∞—Å–æ–≤ (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å)
    now = datetime.now(timezone.utc)
    cutoff = now - timedelta(hours=48)

    fresh = []
    for it in news:
        if not isinstance(it, dict):
            continue
        url = extract_url(it)
        title = extract_title(it)
        if not url or not title:
            continue
        if url in used_urls:
            continue
        if not is_on_topic(it):
            continue

        d = extract_date(it)
        # –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å, –Ω–æ –ª—É—á—à–µ –Ω–∏–∂–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        score = 0
        if d:
            if d < cutoff:
                continue
            # —á–µ–º –Ω–æ–≤–µ–µ ‚Äî —Ç–µ–º –≤—ã—à–µ —à–∞–Ω—Å –ø–æ–ø–∞—Å—Ç—å
            age_hours = (now - d).total_seconds() / 3600
            score = max(0, int(100 - age_hours))
        else:
            score = 10

        fresh.append((score, it))

    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ‚Äú–Ω–æ–≤–∏–∑–Ω–µ‚Äù, –Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Ä–∞–Ω–¥–æ–º–æ–º –∏–∑ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏
    fresh.sort(key=lambda x: x[0], reverse=True)
    top_pool = [it for _, it in fresh[: max(20, PICK_N * 4)]]

    if len(top_pool) <= PICK_N:
        return top_pool

    return random.sample(top_pool, PICK_N)

def esc_html(s: str) -> str:
    return html.escape(s, quote=False)

def make_digest_post(items: list[dict]) -> str:
    today = datetime.now().strftime("%d.%m.%Y")
    lines = [f"üöõ <b>–ì–ª–∞–≤–Ω–æ–µ –ø–æ —Ç—è–≥–∞—á–∞–º –∏ –ø–æ–ª—É–ø—Ä–∏—Ü–µ–ø–∞–º ‚Äî {today}</b>", ""]

    for i, it in enumerate(items, 1):
        title = esc_html(extract_title(it))
        url = extract_url(it)

        meaning = "–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç: –ø—Ä–æ–≤–µ—Ä—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ü–µ–Ω—ã, —Å—Ä–æ–∫–∏ –ø–æ—Å—Ç–∞–≤–æ–∫ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é. –°—Å—ã–ª–∫–∏ ‚Äî –Ω–∏–∂–µ."

        lines.append(f"{i}Ô∏è‚É£ <b>{title}</b>")
        lines.append(esc_html(meaning))
        lines.append(f"üîó {url}")
        lines.append("")

    lines.append("üìå –≠—Ç–æ –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è —Å–≤–æ–¥–∫–∞: –±–µ–∑ —Å–ø–∞–º–∞, —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ–µ + –≤—ã–≤–æ–¥—ã.")
    return "\n".join(lines).strip()

def tg_send(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("Missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID")

    api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False,
    }
    r = requests.post(api, json=payload, timeout=30)

    # –ü–æ–∫–∞–∂–µ–º —Ç–µ–ª—É –æ—à–∏–±–∫–∏ Telegram (–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ)
    try:
        j = r.json()
    except Exception:
        j = {"raw": r.text}

    print("Telegram status:", r.status_code)
    print("Telegram response:", j)

    r.raise_for_status()
    if not j.get("ok"):
        raise RuntimeError(f"Telegram API error: {j}")


def main():
    state = load_state()

    # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–≤–æ–π–Ω–æ–≥–æ –ø–æ—Å—Ç–∏–Ω–≥–∞ –≤ –æ–¥–∏–Ω –¥–µ–Ω—å (–µ—Å–ª–∏ workflow –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏–ª–∏)
    today = datetime.now().strftime("%Y-%m-%d")
    if state.get("last_post_date") == today:
        print("Digest already posted today. Exit.")
        return

    used = set(state.get("used_urls", []))
    news = read_news()
    picked = pick_items(news, used)

    if not picked:
        print("No suitable items found (topic/freshness/duplicates). Exit.")
        return

    post = make_digest_post(picked)
    tg_send(post)

    # –æ–±–Ω–æ–≤–ª—è–µ–º state: –∑–∞–ø–æ–º–∏–Ω–∞–µ–º —Å—Å—ã–ª–∫–∏, —á—Ç–æ–±—ã –∑–∞–≤—Ç—Ä–∞ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è
    for it in picked:
        url = extract_url(it)
        if url:
            used.add(url)

    state["used_urls"] = list(used)[-500:]  # –æ–≥—Ä–∞–Ω–∏—á–∏–º –ø–∞–º—è—Ç—å
    state["last_post_date"] = today
    save_state(state)

    print("OK: digest posted.")

if __name__ == "__main__":
    main()
