name: Ingest & Publish

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/30 5-20 * * *"   # пример: каждые 30 мин с 08:00 до 23:59 МСК

permissions:
  contents: write

jobs:
  ingest:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (if any)
        shell: bash
        run: |
          set -eux
          if [ -f "requirements.txt" ]; then pip install -r requirements.txt; fi
          if [ -f "aggregator/requirements.txt" ]; then pip install -r aggregator/requirements.txt; fi
          pip install pyyaml >/dev/null 2>&1 || true

      - name: Quick YAML check (rss length)
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import yaml, pathlib
          p = pathlib.Path('aggregator/sources.yml')
          data = yaml.safe_load(p.read_text(encoding='utf-8'))
          rss = data.get('rss') or []
          print('rss items:', len(rss))
          if rss:
              print('first rss url:', rss[0].get('url'))
          PY

      - name: Run aggregator (keep whatever it produces)
        shell: bash
        run: |
          set -euxo pipefail
          # Если aggregator/main.py ничего не соберёт — ок, следующий шаг сольёт RSS
          python aggregator/main.py --sources aggregator/sources.yml --output frontend/data/news.json || true

      # ВСЕГДА мерджим ВСЕ RSS из sources.yml поверх текущего news.json
      - name: Merge aggregator output with ALL RSS in sources.yml
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import json, pathlib, urllib.request, urllib.parse, xml.etree.ElementTree as ET
          import yaml, time, hashlib

          OUT = pathlib.Path("frontend/data/news.json")
          SRC = pathlib.Path("aggregator/sources.yml")

          # читаем то, что уже есть (от агрегатора)
          data = []
          if OUT.exists():
              try:
                  data = json.loads(OUT.read_text(encoding="utf-8"))
              except Exception:
                  data = []

          seen = set((it.get("url") or "").strip() for it in data if (it.get("url") or "").strip())

          def fetch(url: str) -> bytes:
              # нормальный UA — иначе многие RU-сайты отдают 404/403/502
              req = urllib.request.Request(
                  url,
                  headers={
                      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                                    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36 NewsBot/1.0",
                      "Accept": "application/rss+xml,application/xml;q=0.9,*/*;q=0.8",
                  }
              )
              with urllib.request.urlopen(req, timeout=30) as r:
                  return r.read()

          def pass_keywords(text: str, kws):
              if not kws:
                  return True
              t = (text or "").lower()
              return any(k.lower() in t for k in kws)

          def add_item(title, link, summary, pub, img):
              link = (link or "").strip()
              if link and link in seen:
                  return
              if link:
                  seen.add(link)
              dom = urllib.parse.urlparse(link).netloc if link else ""
              data.append({
                  "id": link or hashlib.md5((title+summary).encode("utf-8","ignore")).hexdigest(),
                  "title": title or "",
                  "url": link,
                  "domain": dom,
                  "summary": summary or "",
                  "image": img or "",
                  "date": pub or ""
              })

          cfg = yaml.safe_load(SRC.read_text(encoding="utf-8"))
          feeds = cfg.get("rss") or []

          for f in feeds:
              url = (f.get("url") or "").strip()
              name = f.get("name") or url
              kws  = f.get("include_keywords") or []
              if not url:
                  continue
              try:
                  xml = fetch(url)
                  root = ET.fromstring(xml)
                  cnt = 0
                  for it in root.findall(".//item"):
                      title = (it.findtext("title") or "").strip()
                      link  = (it.findtext("link")  or "").strip()
                      desc  = (it.findtext("description") or "").strip()
                      pub   = (it.findtext("pubDate") or "").strip()
                      text  = f"{title}\n{desc}"
                      if not pass_keywords(text, kws):
                          continue
                      img = ""
                      enc = it.find("enclosure")
                      if enc is not None and enc.get("url"):
                          img = enc.get("url")
                      add_item(title, link, desc, pub, img)
                      cnt += 1
                  print(f"[RSS] {name}: {cnt}")
              except Exception as e:
                  print(f"[RSS] {name}: ERROR {e}")

          # сортировка по дате (best-effort)
          def ts(x):
              s = x.get("date") or ""
              try:
                  return time.mktime(time.strptime(s[:25], "%a, %d %b %Y %H:%M:%S"))
              except Exception:
                  return 0

          data.sort(key=ts, reverse=True)

          OUT.parent.mkdir(parents=True, exist_ok=True)
          OUT.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
          print("Merged feed size:", len(data))
          PY

      - name: Preview output
        shell: bash
        run: |
          set -eux
          wc -c frontend/data/news.json || true
          python - <<'PY'
          import json
          arr=json.load(open('frontend/data/news.json','rb'))
          print('items:', len(arr))
          for it in arr[:5]:
              print('-', (it.get('title') or '')[:120])
          PY

      - name: Commit & push changes
        shell: bash
        run: |
          set -eux
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git diff --staged --quiet || git commit -m "chore: ingest feed → merge aggregator output with RSS"
          git push
