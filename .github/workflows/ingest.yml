name: Ingest & Publish

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/30 5-20 * * *"   # пример автообновления

permissions:
  contents: write

jobs:
  ingest:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (if any)
        shell: bash
        run: |
          set -eux
          if [ -f "requirements.txt" ]; then pip install -r requirements.txt; fi
          if [ -f "aggregator/requirements.txt" ]; then pip install -r aggregator/requirements.txt; fi
          pip install pyyaml beautifulsoup4 lxml >/dev/null 2>&1 || true

      - name: Quick YAML check (rss length)
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import yaml, pathlib
          p = pathlib.Path('aggregator/sources.yml')
          data = yaml.safe_load(p.read_text(encoding='utf-8'))
          rss = data.get('rss') or []
          print('rss items:', len(rss))
          if rss:
              print('first rss url:', rss[0].get('url'))
          PY

      - name: Run aggregator (keep whatever it produces)
        shell: bash
        run: |
          set -euxo pipefail
          # если агрегатор ничего не соберёт — ок, дальше сольём RSS
          python aggregator/main.py --sources aggregator/sources.yml --output frontend/data/news.json || true

      - name: Merge aggregator output with ALL RSS in sources.yml (+image rescue)
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import json, pathlib, time, ssl, re
          import urllib.request, urllib.parse
          from urllib.error import HTTPError, URLError
          import xml.etree.ElementTree as ET
          import yaml, hashlib
          from bs4 import BeautifulSoup

          OUT = pathlib.Path("frontend/data/news.json")
          SRC = pathlib.Path("aggregator/sources.yml")

          # ---- базовые данные
          data = []
          if OUT.exists():
              try:
                  data = json.loads(OUT.read_text(encoding="utf-8"))
              except Exception:
                  data = []

          seen = set((it.get("url") or "").strip() for it in data if (it.get("url") or "").strip())

          UA_HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                            "(KHTML, like Gecko) Chrome/124.0 Safari/537.36 NewsBot/1.0",
              "Accept": "application/rss+xml,application/xml;q=0.9,*/*;q=0.8",
              "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
              "Referer": "https://www.google.com/",
          }

          HTML_HEADERS = UA_HEADERS | {
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
          }

          ssl_ctx = ssl.create_default_context()
          ssl_ctx.check_hostname = False
          ssl_ctx.verify_mode = ssl.CERT_NONE

          def open_url(url: str, headers: dict, timeout: int = 30):
              req = urllib.request.Request(url, headers=headers)
              return urllib.request.urlopen(req, timeout=timeout, context=ssl_ctx)

          def fetch_with_retries(url: str, headers: dict, retries: int = 2, backoff: float = 1.7) -> bytes:
              last = None
              for i in range(retries + 1):
                  try:
                      with open_url(url, headers) as r:
                          return r.read()
                  except Exception as e:
                      last = e
                      time.sleep(backoff ** i)
              raise last

          # --- нормализация абсолютного URL
          def abs_url(base: str, maybe: str) -> str:
              if not maybe: return ""
              return urllib.parse.urljoin(base, maybe)

          # --- парс картинок из HTML: og:image / twitter:image / itemprop=image
          META_IMG_KEYS = [
              ("property","og:image"),
              ("name","og:image"),
              ("name","twitter:image"),
              ("property","twitter:image"),
              ("itemprop","image"),
          ]

          def extract_meta_image(html: bytes, base: str) -> str:
              try:
                  soup = BeautifulSoup(html, "lxml")
              except Exception:
                  soup = BeautifulSoup(html, "html.parser")
              # стандартные мета
              for attr, val in META_IMG_KEYS:
                  tag = soup.find("meta", attrs={attr: val})
                  if tag and (tag.get("content") or tag.get("value")):
                      img = tag.get("content") or tag.get("value")
                      img = abs_url(base, img.strip())
                      if img: return img
              # fallback: первая картинка на странице
              img_tag = soup.find("img", src=True)
              if img_tag:
                  return abs_url(base, img_tag["src"].strip())
              return ""

          def pass_keywords(text: str, kws):
              if not kws: return True
              t = (text or "").lower()
              return any(k.lower() in t for k in kws)

          def add_item(acc, title, link, summary, pub, img):
              link = (link or "").strip()
              if link and link in seen:
                  return
              if link:
                  seen.add(link)
              dom = urllib.parse.urlparse(link).netloc if link else ""
              acc.append({
                  "id": link or hashlib.md5((title+summary).encode("utf-8","ignore")).hexdigest(),
                  "title": title or "",
                  "url": link,
                  "domain": dom,
                  "summary": summary or "",
                  "image": img or "",
                  "date": pub or ""
              })

          cfg = yaml.safe_load(SRC.read_text(encoding="utf-8"))
          feeds = cfg.get("rss") or []

          # лимит «глубокой подкачки» HTML (за прогон), чтобы не долбить сайты
          MAX_HTML_SCRAPES = 40
          html_scraped = 0

          appended = 0
          for f in feeds:
              url = (f.get("url") or "").strip()
              name = f.get("name") or url
              kws  = f.get("include_keywords") or []
              if not url:
                  continue
              try:
                  xml = fetch_with_retries(url, UA_HEADERS)
                  root = ET.fromstring(xml)

                  # media namespace может быть любым, используем wildcard
                  def media_content(el):
                      m = el.find(".//{*}content")
                      if m is not None and m.get("url"):
                          return m.get("url")
                      return ""

                  cnt = 0
                  for it in root.findall(".//item"):
                      title = (it.findtext("title") or "").strip()
                      link  = (it.findtext("link")  or "").strip()
                      desc  = (it.findtext("description") or "").strip()
                      pub   = (it.findtext("pubDate") or "").strip()
                      text  = f"{title}\n{desc}"
                      if not pass_keywords(text, kws):
                          continue

                      img = ""
                      # 1) enclosure
                      enc = it.find("enclosure")
                      if enc is not None and enc.get("url"):
                          img = enc.get("url")

                      # 2) media:content
                      if not img:
                          mc = media_content(it)
                          if mc:
                              img = mc

                      # 3) если Google News/другие не дали — подкачиваем HTML статьи и вытаскиваем og:image
                      if not img and link and html_scraped < MAX_HTML_SCRAPES:
                          try:
                              html = fetch_with_retries(link, HTML_HEADERS, retries=2)
                              img = extract_meta_image(html, link)
                              html_scraped += 1
                          except Exception:
                              pass

                      # нормализуем URL
                      img = abs_url(link, img)

                      add_item(data, title, link, desc, pub, img)
                      cnt += 1
                  appended += cnt
                  print(f"[RSS] {name}: {cnt}")
              except Exception as e:
                  print(f"[RSS] {name}: ERROR {e}")

          # сортировка
          def ts(x):
              s = x.get("date") or ""
              try:
                  return time.mktime(time.strptime(s[:25], "%a, %d %b %Y %H:%M:%S"))
              except Exception:
                  return 0

          data.sort(key=ts, reverse=True)
          OUT.parent.mkdir(parents=True, exist_ok=True)
          OUT.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
          print("Merged feed size:", len(data), "| appended:", appended, "| html_scraped:", html_scraped)
          PY

      - name: Preview output
        shell: bash
        run: |
          set -eux
          wc -c frontend/data/news.json || true
          python - <<'PY'
          import json
          arr=json.load(open('frontend/data/news.json','rb'))
          print('items:', len(arr))
          for it in arr[:5]:
              print('-', (it.get('title') or '')[:120], '| img:', bool(it.get('image')))
          PY

      - name: Commit & push changes
        shell: bash
        run: |
          set -eux
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git diff --staged --quiet || git commit -m "chore: ingest + fetch og:image for Google News items"
          git push
