name: Ingest & Publish

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "*/30 5-20 * * *"

permissions:
  contents: write

jobs:
  ingest:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (if any)
        shell: bash
        run: |
          set -eux
          if [ -f "requirements.txt" ]; then pip install -r requirements.txt; fi
          if [ -f "aggregator/requirements.txt" ]; then pip install -r aggregator/requirements.txt; fi
          pip install pyyaml beautifulsoup4 lxml >/dev/null 2>&1 || true

      - name: Quick YAML check (rss length)
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import yaml, pathlib
          p = pathlib.Path('aggregator/sources.yml')
          data = yaml.safe_load(p.read_text(encoding='utf-8'))
          rss = data.get('rss') or []
          print('rss items:', len(rss))
          if rss:
              print('first rss url:', rss[0].get('url'))
          PY

      - name: Run aggregator (keep whatever it produces)
        shell: bash
        run: |
          set -euxo pipefail
          python aggregator/main.py --sources aggregator/sources.yml --output frontend/data/news.json || true

      - name: Merge aggregator output with ALL RSS in sources.yml (+image rescue)
        shell: bash
        run: |
          set -eux
          python - <<'PY'
          import json, pathlib, time, ssl, re
          import urllib.request, urllib.parse
          from urllib.error import HTTPError, URLError
          import xml.etree.ElementTree as ET
          import yaml, hashlib
          from bs4 import BeautifulSoup

          OUT = pathlib.Path("frontend/data/news.json")
          SRC = pathlib.Path("aggregator/sources.yml")

          data = []
          if OUT.exists():
              try:
                  data = json.loads(OUT.read_text(encoding="utf-8"))
              except Exception:
                  data = []

          seen = set((it.get("url") or "").strip() for it in data if (it.get("url") or "").strip())

          UA_HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                            "(KHTML, like Gecko) Chrome/124.0 Safari/537.36 NewsBot/1.2",
              "Accept": "application/rss+xml,application/xml;q=0.9,*/*;q=0.8",
              "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
              "Referer": "https://www.google.com/",
          }
          HTML_HEADERS = UA_HEADERS | {
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
          }

          ssl_ctx = ssl.create_default_context()
          ssl_ctx.check_hostname = False
          ssl_ctx.verify_mode = ssl.CERT_NONE

          def open_url(url: str, headers: dict, timeout: int = 30):
              req = urllib.request.Request(url, headers=headers)
              return urllib.request.urlopen(req, timeout=timeout, context=ssl_ctx)

          def fetch_with_retries(url: str, headers: dict, retries: int = 2, backoff: float = 1.7) -> bytes:
              last = None
              for i in range(retries + 1):
                  try:
                      with open_url(url, headers) as r:
                          return r.read()
                  except Exception as e:
                      last = e
                      time.sleep(backoff ** i)
              raise last

          def abs_url(base: str, maybe: str) -> str:
              if not maybe: return ""
              return urllib.parse.urljoin(base, maybe)

          META_IMG_KEYS = [
              ("property","og:image"),
              ("name","og:image"),
              ("name","twitter:image"),
              ("property","twitter:image"),
              ("itemprop","image"),
          ]

          def extract_meta_image(html: bytes, base: str) -> str:
              try:
                  soup = BeautifulSoup(html, "lxml")
              except Exception:
                  soup = BeautifulSoup(html, "html.parser")
              for attr, val in META_IMG_KEYS:
                  tag = soup.find("meta", attrs={attr: val})
                  if tag and (tag.get("content") or tag.get("value")):
                      img = tag.get("content") or tag.get("value")
                      img = abs_url(base, img.strip())
                      if img: return img
              img_tag = soup.find("img", src=True)
              if img_tag:
                  return abs_url(base, img_tag["src"].strip())
              return ""

          def pass_keywords(text: str, kws):
              if not kws: return True
              t = (text or "").lower()
              return any(k.lower() in t for k in kws)

          # --- google placeholder?
          def is_google_placeholder(url: str) -> bool:
              try:
                  host = urllib.parse.urlparse(url).netloc
              except Exception:
                  return False
              host = host.lower()
              return ("news.google.com" in host) or ("gstatic" in host)

          # --- распаковываем ссылку издателя из Google News
          def resolve_publisher_link(link: str, desc_html: str, resolve_left: list) -> str:
              try:
                  host = urllib.parse.urlparse(link).netloc
              except Exception:
                  host = ""
              if "news.google.com" not in host:
                  return link

              # 1) из <description> — берём первый <a href>, который не news.google.com
              if desc_html:
                  try:
                      soup = BeautifulSoup(desc_html, "lxml")
                  except Exception:
                      soup = BeautifulSoup(desc_html, "html.parser")
                  for a in soup.find_all("a", href=True):
                      u = a["href"]
                      try:
                          if "news.google.com" not in urllib.parse.urlparse(u).netloc:
                              return u
                      except Exception:
                          pass

              # 2) пробуем распарсить url/u в query
              try:
                  qs = urllib.parse.parse_qs(urllib.parse.urlparse(link).query)
                  for key in ("url","u"):
                      if key in qs and qs[key]:
                          u = qs[key][0]
                          if "news.google.com" not in urllib.parse.urlparse(u).netloc:
                              return u
              except Exception:
                  pass

              # 3) если лимит не исчерпан — делаем GET и берём финальный Location
              if resolve_left[0] > 0:
                  try:
                      req = urllib.request.Request(link, headers=HTML_HEADERS)
                      with urllib.request.urlopen(req, timeout=15, context=ssl_ctx) as r:
                          final = r.geturl()
                          if "news.google.com" not in urllib.parse.urlparse(final).netloc:
                              resolve_left[0] -= 1
                              return final
                  except Exception:
                      pass

              return link

          def add_item(acc, title, link, summary, pub, img):
              link = (link or "").strip()
              if link and link in seen:
                  return
              if link:
                  seen.add(link)
              dom = urllib.parse.urlparse(link).netloc if link else ""
              acc.append({
                  "id": link or hashlib.md5((title+summary).encode("utf-8","ignore")).hexdigest(),
                  "title": title or "",
                  "url": link,
                  "domain": dom,
                  "summary": summary or "",
                  "image": img or "",
                  "date": pub or ""
              })

          cfg = yaml.safe_load(SRC.read_text(encoding="utf-8"))
          feeds = cfg.get("rss") or []

          MAX_HTML_SCRAPES = 300         # сколько страниц статьи открываем для og:image
          html_scraped = 0
          RESOLVE_LIMIT = 150            # сколько google-ссылок разрешаем «раскрутить»
          resolve_left = [RESOLVE_LIMIT]

          appended = 0
          for f in feeds:
              url = (f.get("url") or "").strip()
              name = f.get("name") or url
              kws  = f.get("include_keywords") or []
              if not url:
                  continue
              try:
                  xml = fetch_with_retries(url, UA_HEADERS)
                  root = ET.fromstring(xml)

                  def media_content(el):
                      m = el.find(".//{*}content")
                      if m is not None and m.get("url"):
                          return m.get("url")
                      return ""
                  def media_thumbnail(el):
                      m = el.find(".//{*}thumbnail")
                      if m is not None and m.get("url"):
                          return m.get("url")
                      return ""
                  def image_tag(el):
                      m = el.find("image")
                      if m is not None:
                          u = m.findtext("url")
                          if u: return u
                      return ""

                  cnt = 0
                  for it in root.findall(".//item"):
                      title = (it.findtext("title") or "").strip()
                      link  = (it.findtext("link")  or "").strip()
                      desc  = (it.findtext("description") or "").strip()
                      pub   = (it.findtext("pubDate") or "").strip()

                      real_link = resolve_publisher_link(link, desc, resolve_left) or link

                      text  = f"{title}\n{desc}"
                      if not pass_keywords(text, kws):
                          continue

                      img = ""
                      enc = it.find("enclosure")
                      if enc is not None and enc.get("url"):
                          img = enc.get("url")
                      if not img:
                          mc = media_content(it)
                          if mc: img = mc
                      if not img:
                          tn = media_thumbnail(it)
                          if tn: img = tn
                      if not img:
                          im = image_tag(it)
                          if im: img = im

                      # отбрасываем «синие заглушки» от Google
                      if img and is_google_placeholder(img):
                          img = ""

                      if not img and real_link and html_scraped < MAX_HTML_SCRAPES:
                          try:
                              html = fetch_with_retries(real_link, HTML_HEADERS, retries=2)
                              img = extract_meta_image(html, real_link)
                              html_scraped += 1
                          except Exception:
                              pass

                      img = abs_url(real_link, img)

                      add_item(data, title, real_link, desc, pub, img)
                      cnt += 1
                  appended += cnt
                  print(f"[RSS] {name}: {cnt}")
              except Exception as e:
                  print(f"[RSS] {name}: ERROR {e}")

          def ts(x):
              s = x.get("date") or ""
              try:
                  return time.mktime(time.strptime(s[:25], "%a, %d %b %Y %H:%M:%S"))
              except Exception:
                  return 0

          data.sort(key=ts, reverse=True)
          OUT.parent.mkdir(parents=True, exist_ok=True)
          OUT.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
          print("Merged feed size:", len(data), "| appended:", appended, "| html_scraped:", html_scraped, "| resolved:", RESOLVE_LIMIT - resolve_left[0])
          PY

      - name: Preview output
        shell: bash
        run: |
          set -eux
          wc -c frontend/data/news.json || true
          python - <<'PY'
          import json, urllib.parse
          arr=json.load(open('frontend/data/news.json','rb'))
          print('items:', len(arr))
          c_with_img=sum(1 for x in arr if x.get('image'))
          print('with images:', c_with_img)
          for it in arr[:10]:
            host = urllib.parse.urlparse(it.get('url') or '').netloc
            print('-', (it.get('title') or '')[:90], '| host:', host, '| img:', bool(it.get('image')))
          PY

      - name: Commit & push changes
        shell: bash
        run: |
          set -eux
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git diff --staged --quiet || git commit -m "feat: resolve Google links to publisher + drop google placeholders + stronger og:image"
          git push
